# Data-science-progress
Data Science learning journey.

Current project Underway: AgriNet Challenge 
Description: Refer the site Instead, but the general dataset for this challenge consists crop types of agricultural fields in four states of Uttar Pradesh, Rajasthan, Odisha and Bihar in northern India. There are 13 different classes in the dataset including Fallow land and 12 crop types of Wheat, Mustard, Lentil, Green pea, Sugarcane, Garlic, Maize, Gram, Coriander, Potato, Bersem, and Rice. the dataset is the inclusion of Sentinel-2 images are stored as tif files and is stored and accessed using Api, generated by the Site Radient_mlhub. 
Objective: To find the best predictions on the classification of crop type using 12 bands of color and pixels as a feature to determine the crop type. The challenge also promotes usage of publicly available sources of libraries and code. 
Methodology of approach: 
1. initial import of base code and the dataset sourced using Radient_mlhub Api.( make a different folder to accomodate the steps and proceedures that you may have taken to complete this task 
2. Run the base code to make sure it is running before it is to be submitted to the portal. (Make note of the current structure of the model, where the datasets are being split into test, training and cross validation test)
3. the next step would be to plan how to improve the model by setting up parameters and hyperter tuning to provide better predictions for the base code. just like a backbone to any human being, hyperparameter tuning is done to provide structure to the training set. SO LETS GET INTO THE SCIKIT LEARN MODEULE. 
     1. so far under hyperparameter tuning for random forest I have encountered the definition: Hp tuning is like the setting of an ml algo which is adjusting itself to optimise the algo. best explained usng the example of having to split the dataset betweeen 3 types that is 
                                                                                         Test set        Training set           Cross validation set
            A Brief Explanation of Hyperparameter Tuning
The best way to think about hyperparameters is like the settings of an algorithm that can be adjusted to optimize performance, just as we might turn the knobs of an AM radio to get a clear signal (or your parents might have!). While model parameters are learned during training — such as the slope and intercept in a linear regression — hyperparameters must be set by the data scientist before training. In the case of a random forest, hyperparameters include the 
*number of decision trees in the forest and the *number of features considered by each tree when splitting a node. Return this output with a dataset on github. find a dataset to practice.




Xtras on Cross entrpoy:
1.What Is Cross-Entropy?
2. Cross-Entropy Versus KL Divergence
3. How to Calculate Cross-Entropy
3.1 Two Discrete Probability Distributions
3.2 Calculate Cross-Entropy Between Distributions
3.3 Calculate Cross-Entropy Between a Distribution and Itself
3.4 Calculate Cross-Entropy Using KL Divergence
4. Cross-Entropy as a Loss Function
4.1 Calculate Entropy for Class Labels
4.2 Calculate Cross-Entropy Between Class Labels and Probabilities
4.3 Calculate Cross-Entropy Using Keras
4.4 Intuition for Cross-Entropy on Predicted Probabilities
5. Cross-Entropy Versus Log Loss
5.1 Log Loss is the Negative Log Likelihood
5.2 Log Loss and Cross Entropy Calculate the Same Thing
